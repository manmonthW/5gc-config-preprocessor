# æ€§èƒ½ä¼˜åŒ–æŒ‡å—

## ğŸ“Š æ€§èƒ½åŸºå‡†

### å½“å‰æ€§èƒ½æŒ‡æ ‡
- **å¤„ç†é€Ÿåº¦**: ~50MB/åˆ†é’Ÿï¼ˆå•çº¿ç¨‹ï¼‰
- **å†…å­˜å ç”¨**: <2GBï¼ˆ100MBæ–‡ä»¶ï¼‰
- **CPUä½¿ç”¨ç‡**: å•æ ¸~80%
- **è„±æ•å‡†ç¡®ç‡**: >95%

## ğŸš€ ä¼˜åŒ–ç­–ç•¥

### 1. å¹¶è¡Œå¤„ç†ä¼˜åŒ–

#### å¤šè¿›ç¨‹å¤„ç†
```python
from multiprocessing import Pool, cpu_count

class ParallelPreProcessor:
    def __init__(self, num_workers=None):
        self.num_workers = num_workers or cpu_count()
    
    def process_files_parallel(self, file_list):
        with Pool(self.num_workers) as pool:
            results = pool.map(self.process_single_file, file_list)
        return results
```

**é…ç½®å»ºè®®**ï¼š
```yaml
performance:
  parallel_processing: true
  num_workers: 4  # æ ¹æ®CPUæ ¸å¿ƒæ•°è°ƒæ•´
  batch_size: 10
```

### 2. å†…å­˜ä¼˜åŒ–

#### æµå¼å¤„ç†å¤§æ–‡ä»¶
```python
def process_large_file_streaming(file_path, chunk_size=1024*1024):
    """æµå¼å¤„ç†å¤§æ–‡ä»¶ï¼Œå‡å°‘å†…å­˜å ç”¨"""
    with open(file_path, 'r', encoding='utf-8') as f:
        while True:
            chunk = f.read(chunk_size)
            if not chunk:
                break
            # å¤„ç†å—
            yield process_chunk(chunk)
```

#### å†…å­˜æ˜ å°„
```python
import mmap

def process_with_mmap(file_path):
    """ä½¿ç”¨å†…å­˜æ˜ å°„å¤„ç†å¤§æ–‡ä»¶"""
    with open(file_path, 'r+b') as f:
        with mmap.mmap(f.fileno(), 0) as mmapped_file:
            # ç›´æ¥æ“ä½œæ˜ å°„çš„å†…å­˜
            content = mmapped_file.read()
            return process_content(content)
```

### 3. ç¼“å­˜ä¼˜åŒ–

#### LRUç¼“å­˜
```python
from functools import lru_cache

@lru_cache(maxsize=128)
def compile_pattern(pattern_str):
    """ç¼“å­˜ç¼–è¯‘çš„æ­£åˆ™è¡¨è¾¾å¼"""
    return re.compile(pattern_str)

class CachedDesensitizer:
    def __init__(self):
        self.cache = {}
    
    @lru_cache(maxsize=1000)
    def desensitize_value(self, value, rule_type):
        """ç¼“å­˜è„±æ•ç»“æœ"""
        return self._apply_rule(value, rule_type)
```

#### Redisç¼“å­˜ï¼ˆåˆ†å¸ƒå¼åœºæ™¯ï¼‰
```python
import redis
import json

class RedisCache:
    def __init__(self):
        self.redis_client = redis.Redis(
            host='localhost',
            port=6379,
            decode_responses=True
        )
    
    def get_or_compute(self, key, compute_func):
        # å°è¯•ä»ç¼“å­˜è·å–
        cached = self.redis_client.get(key)
        if cached:
            return json.loads(cached)
        
        # è®¡ç®—å¹¶ç¼“å­˜
        result = compute_func()
        self.redis_client.setex(
            key, 
            3600,  # TTL: 1å°æ—¶
            json.dumps(result)
        )
        return result
```

### 4. ç®—æ³•ä¼˜åŒ–

#### æ‰¹é‡æ­£åˆ™åŒ¹é…
```python
import re

class OptimizedMatcher:
    def __init__(self, patterns):
        # åˆå¹¶å¤šä¸ªæ¨¡å¼ä¸ºä¸€ä¸ª
        self.combined_pattern = re.compile(
            '|'.join(f'(?P<{name}>{pattern})' 
                    for name, pattern in patterns.items())
        )
    
    def match_all(self, text):
        """ä¸€æ¬¡åŒ¹é…å¤šä¸ªæ¨¡å¼"""
        matches = {}
        for match in self.combined_pattern.finditer(text):
            for name, value in match.groupdict().items():
                if value:
                    if name not in matches:
                        matches[name] = []
                    matches[name].append(value)
        return matches
```

#### ä½¿ç”¨Aho-Corasickç®—æ³•è¿›è¡Œå¤šæ¨¡å¼åŒ¹é…
```python
import pyahocorasick

class FastKeywordMatcher:
    def __init__(self, keywords):
        self.automaton = pyahocorasick.Automaton()
        for idx, key in enumerate(keywords):
            self.automaton.add_word(key, (idx, key))
        self.automaton.make_automaton()
    
    def find_all(self, text):
        """å¿«é€ŸæŸ¥æ‰¾æ‰€æœ‰å…³é”®è¯"""
        matches = []
        for end_index, (idx, keyword) in self.automaton.iter(text):
            start_index = end_index - len(keyword) + 1
            matches.append((keyword, start_index, end_index))
        return matches
```

### 5. I/Oä¼˜åŒ–

#### å¼‚æ­¥I/O
```python
import asyncio
import aiofiles

async def process_file_async(file_path):
    """å¼‚æ­¥æ–‡ä»¶å¤„ç†"""
    async with aiofiles.open(file_path, 'r') as f:
        content = await f.read()
        # å¼‚æ­¥å¤„ç†
        result = await process_content_async(content)
        return result

async def process_multiple_files(file_paths):
    """å¹¶å‘å¤„ç†å¤šä¸ªæ–‡ä»¶"""
    tasks = [process_file_async(fp) for fp in file_paths]
    results = await asyncio.gather(*tasks)
    return results
```

#### æ‰¹é‡å†™å…¥
```python
class BatchWriter:
    def __init__(self, batch_size=1000):
        self.batch_size = batch_size
        self.buffer = []
    
    def write(self, data):
        self.buffer.append(data)
        if len(self.buffer) >= self.batch_size:
            self.flush()
    
    def flush(self):
        if self.buffer:
            # æ‰¹é‡å†™å…¥
            with open(self.output_file, 'a') as f:
                f.write('\n'.join(self.buffer))
            self.buffer = []
```

### 6. æ•°æ®åº“ä¼˜åŒ–ï¼ˆå¦‚æœä½¿ç”¨ï¼‰

#### è¿æ¥æ± 
```python
from contextlib import contextmanager
import sqlite3
from queue import Queue

class ConnectionPool:
    def __init__(self, db_path, max_connections=10):
        self.db_path = db_path
        self.pool = Queue(maxsize=max_connections)
        for _ in range(max_connections):
            conn = sqlite3.connect(db_path)
            self.pool.put(conn)
    
    @contextmanager
    def get_connection(self):
        conn = self.pool.get()
        try:
            yield conn
        finally:
            self.pool.put(conn)
```

#### æ‰¹é‡æ’å…¥
```python
def batch_insert(data_list, batch_size=1000):
    """æ‰¹é‡æ’å…¥æ•°æ®"""
    with connection_pool.get_connection() as conn:
        cursor = conn.cursor()
        for i in range(0, len(data_list), batch_size):
            batch = data_list[i:i+batch_size]
            cursor.executemany(
                "INSERT INTO configs VALUES (?, ?, ?)",
                batch
            )
        conn.commit()
```

### 7. åˆ†å¸ƒå¼å¤„ç†

#### ä½¿ç”¨Celeryè¿›è¡Œä»»åŠ¡é˜Ÿåˆ—
```python
from celery import Celery

app = Celery('config_processor', 
             broker='redis://localhost:6379',
             backend='redis://localhost:6379')

@app.task
def process_config_task(file_path):
    """å¼‚æ­¥å¤„ç†ä»»åŠ¡"""
    processor = ConfigPreProcessor()
    return processor.process_file(file_path)

# ä½¿ç”¨
from celery import group

def process_files_distributed(file_paths):
    """åˆ†å¸ƒå¼å¤„ç†å¤šä¸ªæ–‡ä»¶"""
    job = group(
        process_config_task.s(fp) for fp in file_paths
    )
    result = job.apply_async()
    return result.get()
```

## ğŸ“ˆ æ€§èƒ½ç›‘æ§

### æ€§èƒ½åˆ†æå·¥å…·
```python
import cProfile
import pstats
from memory_profiler import profile

# CPUæ€§èƒ½åˆ†æ
def profile_cpu():
    profiler = cProfile.Profile()
    profiler.enable()
    
    # æ‰§è¡Œä»£ç 
    process_file("large_config.txt")
    
    profiler.disable()
    stats = pstats.Stats(profiler)
    stats.sort_stats('cumulative')
    stats.print_stats(20)

# å†…å­˜åˆ†æ
@profile
def process_with_memory_profile(file_path):
    processor = ConfigPreProcessor()
    return processor.process_file(file_path)
```

### å®æ—¶ç›‘æ§
```python
import psutil
import time

class PerformanceMonitor:
    def __init__(self):
        self.process = psutil.Process()
    
    def monitor(self, interval=1):
        while True:
            cpu_percent = self.process.cpu_percent()
            memory_info = self.process.memory_info()
            
            print(f"CPU: {cpu_percent}%")
            print(f"Memory: {memory_info.rss / 1024 / 1024:.2f} MB")
            
            time.sleep(interval)
```

## ğŸ¯ ä¼˜åŒ–å»ºè®®

### æ ¹æ®æ–‡ä»¶å¤§å°é€‰æ‹©ç­–ç•¥

| æ–‡ä»¶å¤§å° | æ¨èç­–ç•¥ |
|---------|---------|
| < 10MB  | å•çº¿ç¨‹å¤„ç† |
| 10-100MB | å¤šçº¿ç¨‹/æµå¼å¤„ç† |
| 100MB-1GB | å†…å­˜æ˜ å°„/åˆ†å—å¤„ç† |
| > 1GB | åˆ†å¸ƒå¼å¤„ç† |

### æ ¹æ®åœºæ™¯ä¼˜åŒ–

#### å®æ—¶å¤„ç†åœºæ™¯
- ä½¿ç”¨ç¼“å­˜å‡å°‘é‡å¤è®¡ç®—
- å¯ç”¨å¹¶è¡Œå¤„ç†
- ä¼˜åŒ–æ­£åˆ™è¡¨è¾¾å¼

#### æ‰¹é‡å¤„ç†åœºæ™¯
- ä½¿ç”¨å¼‚æ­¥I/O
- æ‰¹é‡å†™å…¥ç»“æœ
- åˆ†å¸ƒå¼ä»»åŠ¡é˜Ÿåˆ—

#### å†…å­˜å—é™åœºæ™¯
- ä½¿ç”¨æµå¼å¤„ç†
- å‡å°å—å¤§å°
- åŠæ—¶é‡Šæ”¾å†…å­˜

## ğŸ”§ è°ƒä¼˜å‚æ•°

### config.yamlä¼˜åŒ–é…ç½®
```yaml
performance:
  # å¹¶è¡Œå¤„ç†
  parallel_processing: true
  num_workers: 8
  
  # å†…å­˜ç®¡ç†
  max_memory_mb: 4096
  chunk_size_kb: 2048
  
  # ç¼“å­˜è®¾ç½®
  cache_enabled: true
  cache_size: 1000
  cache_ttl: 3600
  
  # I/Oä¼˜åŒ–
  batch_write_size: 1000
  async_io: true
  
  # æ­£åˆ™ä¼˜åŒ–
  compile_patterns: true
  pattern_cache_size: 100
```

### ç³»ç»Ÿçº§ä¼˜åŒ–

#### Linuxå†…æ ¸å‚æ•°
```bash
# å¢åŠ æ–‡ä»¶æè¿°ç¬¦é™åˆ¶
ulimit -n 65535

# è°ƒæ•´å†…å­˜å‚æ•°
echo 1 > /proc/sys/vm/swappiness
echo 3 > /proc/sys/vm/drop_caches

# TCPä¼˜åŒ–
sysctl -w net.core.somaxconn=1024
sysctl -w net.ipv4.tcp_max_syn_backlog=1024
```

#### Python GCè°ƒä¼˜
```python
import gc

# è°ƒæ•´åƒåœ¾å›æ”¶é˜ˆå€¼
gc.set_threshold(700, 10, 10)

# åœ¨å¤„ç†å¤§æ–‡ä»¶å‰ç¦ç”¨GC
gc.disable()
process_large_file()
gc.enable()
```

## ğŸ“Š æ€§èƒ½æµ‹è¯•

### åŸºå‡†æµ‹è¯•è„šæœ¬
```python
import time
import os
from statistics import mean, stdev

def benchmark(file_path, iterations=10):
    """è¿è¡ŒåŸºå‡†æµ‹è¯•"""
    times = []
    
    for i in range(iterations):
        start = time.time()
        process_file(file_path)
        elapsed = time.time() - start
        times.append(elapsed)
        
        print(f"Iteration {i+1}: {elapsed:.2f}s")
    
    file_size_mb = os.path.getsize(file_path) / (1024*1024)
    avg_time = mean(times)
    std_dev = stdev(times) if len(times) > 1 else 0
    throughput = file_size_mb / avg_time
    
    print(f"\nåŸºå‡†æµ‹è¯•ç»“æœ:")
    print(f"æ–‡ä»¶å¤§å°: {file_size_mb:.2f} MB")
    print(f"å¹³å‡æ—¶é—´: {avg_time:.2f}s Â± {std_dev:.2f}s")
    print(f"ååé‡: {throughput:.2f} MB/s")
```

## ğŸ¯ ä¼˜åŒ–ç›®æ ‡

### çŸ­æœŸç›®æ ‡ï¼ˆ1ä¸ªæœˆï¼‰
- å¤„ç†é€Ÿåº¦æå‡è‡³ 100MB/åˆ†é’Ÿ
- å†…å­˜å ç”¨é™ä½ 30%
- æ”¯æŒ 500MB æ–‡ä»¶æµç•…å¤„ç†

### ä¸­æœŸç›®æ ‡ï¼ˆ3ä¸ªæœˆï¼‰
- å¤„ç†é€Ÿåº¦è¾¾åˆ° 200MB/åˆ†é’Ÿ
- æ”¯æŒåˆ†å¸ƒå¼å¤„ç†
- å®ç°æ™ºèƒ½ç¼“å­˜æœºåˆ¶

### é•¿æœŸç›®æ ‡ï¼ˆ6ä¸ªæœˆï¼‰
- å¤„ç†é€Ÿåº¦è¾¾åˆ° 500MB/åˆ†é’Ÿ
- æ”¯æŒ 10GB+ è¶…å¤§æ–‡ä»¶
- å®ç°è‡ªé€‚åº”æ€§èƒ½ä¼˜åŒ–

## ğŸ“ ä¼˜åŒ–æ£€æŸ¥æ¸…å•

- [ ] å¯ç”¨å¹¶è¡Œå¤„ç†
- [ ] å®æ–½ç¼“å­˜æœºåˆ¶
- [ ] ä¼˜åŒ–æ­£åˆ™è¡¨è¾¾å¼
- [ ] ä½¿ç”¨æ‰¹é‡I/O
- [ ] é…ç½®å†…å­˜é™åˆ¶
- [ ] å¯ç”¨æ€§èƒ½ç›‘æ§
- [ ] å®šæœŸæ€§èƒ½æµ‹è¯•
- [ ] ä¼˜åŒ–æ•°æ®ç»“æ„
- [ ] å‡å°‘å†…å­˜å¤åˆ¶
- [ ] ä½¿ç”¨æ€§èƒ½åˆ†æå·¥å…·

---

é€šè¿‡ä»¥ä¸Šä¼˜åŒ–ç­–ç•¥ï¼Œå¯ä»¥æ˜¾è‘—æå‡é…ç½®é¢„å¤„ç†æ¨¡å—çš„æ€§èƒ½ã€‚å»ºè®®æ ¹æ®å®é™…ä½¿ç”¨åœºæ™¯å’Œèµ„æºæƒ…å†µï¼Œé€æ­¥å®æ–½ç›¸åº”çš„ä¼˜åŒ–æªæ–½ã€‚
