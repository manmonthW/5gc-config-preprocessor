"""
é…ç½®é¢„å¤„ç†ä¸»æ¨¡å—
æ•´åˆè„±æ•ã€æ ¼å¼è½¬æ¢ã€åˆ†å—ç­‰æ‰€æœ‰é¢„å¤„ç†åŠŸèƒ½
"""

import os
import json
import yaml
import shutil
import logging
import tempfile
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
try:
    from tqdm import tqdm
except ImportError:  # pragma: no cover - fallback when tqdm unavailable
    def tqdm(iterable, **kwargs):
        return iterable

from desensitizer import ConfigDesensitizer
from format_converter import FormatConverter
from chunker import SmartChunker
from metadata_extractor import MetadataExtractor
from utils import resolve_config_path

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class ProcessingResult:
    """é¢„å¤„ç†ç»“æœ"""
    success: bool
    file_path: str
    original_format: str
    processed_files: List[str]
    metadata: Dict
    statistics: Dict
    errors: List[str]
    processing_time: float
    message: str = ""
    output_directory: str = ""
    preferred_output_root: str = ""
    mirrored_output_directory: Optional[str] = None
    mirrored_files: Optional[List[str]] = None
    used_output_fallback: bool = False
    mirror_error: Optional[str] = None
    # Vercel Serverless æ”¯æŒï¼šå†…å­˜æ–‡ä»¶å­˜å‚¨
    memory_files: Optional[Dict[str, bytes]] = None  # {filename: content}

class ConfigPreProcessor:
    """é…ç½®æ–‡ä»¶é¢„å¤„ç†å™¨ä¸»ç±»"""
    
    def __init__(self, config_path: str = "config.yaml"):
        """
        åˆå§‹åŒ–é¢„å¤„ç†å™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config_path = resolve_config_path(config_path)
        self.config = self._load_config(self.config_path)
        
        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        config_path_str = str(self.config_path)
        self.desensitizer = ConfigDesensitizer(config_path_str)
        self.converter = FormatConverter(config_path_str)
        self.chunker = SmartChunker(config_path_str)
        self.metadata_extractor = MetadataExtractor(config_path_str)
        
        # è®¾ç½®è¾“å‡ºç›®å½•ï¼ˆåœ¨åªè¯»ç¯å¢ƒä¸­å›é€€åˆ°ä¸´æ—¶ç›®å½•ï¼‰
        output_settings = self.config.get('output', {})
        base_dir = output_settings.get('base_dir', './output')
        use_timestamp = output_settings.get('create_timestamp_folder', True)
        self.preferred_output_base = self._resolve_output_base(base_dir)
        self.output_base, self.output_dir, self.using_output_fallback = self._prepare_output_directory(
            self.preferred_output_base,
            use_timestamp
        )
        
        # åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯
        self.statistics = {
            'files_processed': 0,
            'total_size_mb': 0,
            'processing_time_seconds': 0,
            'chunks_created': 0,
            'desensitization_count': 0
        }
    
    def _load_config(self, config_path: Path) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    def _resolve_output_base(self, base_dir: str) -> Path:
        """è§£æé…ç½®çš„è¾“å‡ºæ ¹ç›®å½•"""
        resolved_base = Path(base_dir)
        if not resolved_base.is_absolute():
            resolved_base = self.config_path.parent / resolved_base
        return resolved_base
    
    def _prepare_output_directory(self, preferred_base: Path, use_timestamp: bool):
        """ä¸ºå¤„ç†ç»“æœå‡†å¤‡è¾“å‡ºç›®å½•ï¼Œå¿…è¦æ—¶å›é€€åˆ°ä¸´æ—¶ç›®å½•"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S') if use_timestamp else None
        target_dir = preferred_base / timestamp if timestamp else preferred_base
        
        try:
            target_dir.mkdir(parents=True, exist_ok=True)
            return preferred_base, target_dir, False
        except (PermissionError, OSError) as exc:
            fallback_base = Path(tempfile.gettempdir()) / "config_preprocessor_output"
            fallback_dir = fallback_base / timestamp if timestamp else fallback_base
            fallback_dir.mkdir(parents=True, exist_ok=True)
            logger.warning(
                "Output directory %s is not writable (%s). Falling back to %s",
                target_dir,
                exc,
                fallback_dir
            )
            return fallback_base, fallback_dir, True
    
    def process_file(self, file_path: str,
                    desensitize: bool = True,
                    convert_format: bool = True,
                    chunk: bool = True,
                    extract_metadata: bool = True,
                    original_filename: Optional[str] = None,
                    memory_mode: bool = False) -> ProcessingResult:
        """
        å¤„ç†å•ä¸ªé…ç½®æ–‡ä»¶

        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            desensitize: æ˜¯å¦è„±æ•
            convert_format: æ˜¯å¦è½¬æ¢æ ¼å¼
            chunk: æ˜¯å¦åˆ†å—
            extract_metadata: æ˜¯å¦æå–å…ƒæ•°æ®
            original_filename: åŸå§‹æ–‡ä»¶åï¼ˆå¯é€‰ï¼Œç”¨äºè¾“å‡ºç›®å½•å‘½åï¼‰
            memory_mode: Vercel Serverless æ¨¡å¼ - ä¸å†™ç£ç›˜ï¼Œå†…å­˜è¿”å› (é»˜è®¤False)

        Returns:
            å¤„ç†ç»“æœ
        """
        start_time = datetime.now()
        errors = []
        processed_files = []
        metadata = {}
        
        file_output_dir: Optional[Path] = None
        
        try:
            file_path = Path(file_path)
            if not file_path.exists():
                raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            
            # è·å–æ–‡ä»¶ä¿¡æ¯
            file_size_mb = file_path.stat().st_size / (1024 * 1024)

            # ç¡®å®šæ˜¾ç¤ºåç§°
            display_name = original_filename if original_filename else file_path.name
            logger.info(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {display_name} (å¤§å°: {file_size_mb:.2f} MB)")

            # è¯»å–åŸå§‹å†…å®¹ï¼ˆä¿æŒæ ¼å¼ï¼‰
            original_text = self.converter.read_file(str(file_path))
            detected_format = self.converter.detect_format(str(file_path))

            # åˆ›å»ºæ–‡ä»¶ä¸“å±è¾“å‡ºç›®å½•
            # å¦‚æœæä¾›äº†åŸå§‹æ–‡ä»¶åï¼Œä½¿ç”¨åŸå§‹æ–‡ä»¶åï¼›å¦åˆ™ä½¿ç”¨å½“å‰æ–‡ä»¶å
            if original_filename:
                output_dir_name = Path(original_filename).stem
            else:
                output_dir_name = file_path.stem

            file_output_dir = self.output_dir / output_dir_name
            file_output_dir.mkdir(exist_ok=True)

            unified = None

            # Step 1: æ ¼å¼æ£€æµ‹å’Œè½¬æ¢
            if convert_format:
                logger.info("Step 1: æ ¼å¼è½¬æ¢...")
                unified = self.converter.process_file(str(file_path))
                original_format = unified['metadata']['original_format']

                # ä¿å­˜ç»Ÿä¸€æ ¼å¼
                unified_file = file_output_dir / f"{file_path.stem}_unified.json"
                self.converter.save_unified(unified, str(unified_file))
                processed_files.append(str(unified_file))
                logger.info(f"  âœ“ æ ¼å¼è½¬æ¢å®Œæˆ: {original_format} -> unified")
            else:
                original_format = detected_format.value
            
            # Step 2: å…ƒæ•°æ®æå–
            if extract_metadata:
                logger.info("Step 2: å…ƒæ•°æ®æå–...")
                metadata = self.metadata_extractor.extract(original_text)
                
                # ä¿å­˜å…ƒæ•°æ®
                metadata_file = file_output_dir / f"{file_path.stem}_metadata.json"
                with open(metadata_file, 'w', encoding='utf-8') as f:
                    json.dump(metadata, f, ensure_ascii=False, indent=2)
                processed_files.append(str(metadata_file))
                logger.info(f"  âœ“ å…ƒæ•°æ®æå–å®Œæˆ: {len(metadata)} ä¸ªå­—æ®µ")
            
            # Step 3: è„±æ•å¤„ç†
            if desensitize:
                logger.info("Step 3: è„±æ•å¤„ç†...")

                # æ‰§è¡Œè„±æ•ï¼ˆåŸºäºåŸå§‹æ–‡æœ¬ï¼Œä¿æŒæ ¼å¼ï¼‰
                desensitized_content, desensitize_mapping = \
                    self.desensitizer.desensitize_text(original_text)

                # ä¿å­˜è„±æ•åçš„å†…å®¹
                desensitized_file = file_output_dir / f"{file_path.stem}_desensitized.txt"
                with open(desensitized_file, 'w', encoding='utf-8') as f:
                    f.write(desensitized_content)
                processed_files.append(str(desensitized_file))

                # ä¿å­˜è„±æ•æ˜ å°„
                mapping_file = file_output_dir / f"{file_path.stem}_desensitize_mapping.json"
                with open(mapping_file, 'w', encoding='utf-8') as f:
                    json.dump(desensitize_mapping, f, ensure_ascii=False, indent=2)
                processed_files.append(str(mapping_file))

                desensitize_stats = self.desensitizer.get_statistics()
                logger.info(f"  âœ“ è„±æ•å®Œæˆ: {desensitize_stats['total_replacements']} å¤„æ›¿æ¢")

                content_for_chunking = desensitized_content
            else:
                content_for_chunking = original_text
            
            # Step 4: æ™ºèƒ½åˆ†å—
            if chunk:
                logger.info("Step 4: æ™ºèƒ½åˆ†å—...")
                chunks = self.chunker.chunk_text(content_for_chunking)
                
                # ä¿å­˜åˆ†å—
                chunks_dir = file_output_dir / "chunks"
                self.chunker.save_chunks(chunks, str(chunks_dir))
                processed_files.append(str(chunks_dir))
                
                logger.info(f"  âœ“ åˆ†å—å®Œæˆ: ç”Ÿæˆ {len(chunks)} ä¸ªå—")
                self.statistics['chunks_created'] += len(chunks)
            
            # Step 5: ç”Ÿæˆå¤„ç†æŠ¥å‘Š
            logger.info("Step 5: ç”Ÿæˆå¤„ç†æŠ¥å‘Š...")
            report = self._generate_report(
                file_path=str(file_path),
                original_format=original_format,
                metadata=metadata,
                processed_files=processed_files,
                statistics={
                    'file_size_mb': file_size_mb,
                    'chunks_created': len(chunks) if chunk else 0,
                    'desensitization': self.desensitizer.get_statistics() if desensitize else {}
                }
            )
            
            report_file = file_output_dir / f"{file_path.stem}_report.json"
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            processed_files.append(str(report_file))
            logger.info("  âœ“ æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.statistics['files_processed'] += 1
            self.statistics['total_size_mb'] += file_size_mb
            
            # è®¡ç®—å¤„ç†æ—¶é—´
            processing_time = (datetime.now() - start_time).total_seconds()
            self.statistics['processing_time_seconds'] += processing_time
            
            mirror_info = self._mirror_output_if_needed(file_output_dir, processed_files)
            if mirror_info.get('mirrored'):
                logger.info("âœ… è¾“å‡ºç›®å½•å·²åŒæ­¥åˆ°é¡¹ç›® output: %s", mirror_info.get('mirror_dir'))
            elif mirror_info.get('error'):
                logger.warning("âš ï¸ è¾“å‡ºç›®å½•åŒæ­¥å¤±è´¥: %s", mirror_info['error'])
            
            logger.info(f"âœ… æ–‡ä»¶å¤„ç†å®Œæˆ: {file_path}")
            logger.info(f"   å¤„ç†æ—¶é—´: {processing_time:.2f} ç§’")
            logger.info(f"   è¾“å‡ºç›®å½•: {file_output_dir}")

            # Vercel Serverless æ¨¡å¼ï¼šè¯»å–æ‰€æœ‰æ–‡ä»¶åˆ°å†…å­˜
            memory_files = None
            if memory_mode:
                logger.info("ğŸ”„ Vercel æ¨¡å¼ï¼šè¯»å–æ–‡ä»¶åˆ°å†…å­˜...")
                memory_files = {}
                for file_path_str in processed_files:
                    file_path_obj = Path(file_path_str)

                    # å¤„ç†ç›®å½•ï¼ˆchunksï¼‰
                    if file_path_obj.is_dir():
                        # è¯»å–ç›®å½•ä¸‹æ‰€æœ‰æ–‡ä»¶
                        for chunk_file in file_path_obj.rglob('*'):
                            if chunk_file.is_file():
                                relative_name = chunk_file.relative_to(file_output_dir)
                                with open(chunk_file, 'rb') as f:
                                    memory_files[str(relative_name)] = f.read()
                    # å¤„ç†å•ä¸ªæ–‡ä»¶
                    elif file_path_obj.is_file():
                        filename = file_path_obj.name
                        with open(file_path_obj, 'rb') as f:
                            memory_files[filename] = f.read()

                logger.info(f"  âœ“ å·²è¯»å– {len(memory_files)} ä¸ªæ–‡ä»¶åˆ°å†…å­˜")

            return ProcessingResult(
                success=True,
                file_path=str(file_path),
                original_format=original_format,
                processed_files=processed_files,
                metadata=metadata,
                statistics=self.statistics,
                errors=errors,
                processing_time=processing_time,
                message=f"Processing completed successfully in {processing_time:.2f} seconds",
                output_directory=str(file_output_dir),
                preferred_output_root=str(self.preferred_output_base),
                mirrored_output_directory=mirror_info.get('mirror_dir'),
                mirrored_files=mirror_info.get('mirror_files'),
                used_output_fallback=self.using_output_fallback,
                mirror_error=mirror_info.get('error'),
                memory_files=memory_files  # Vercel æ¨¡å¼è¿”å›å†…å­˜æ–‡ä»¶
            )
            
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥: {e}")
            errors.append(str(e))
            
            return ProcessingResult(
                success=False,
                file_path=str(file_path),
                original_format="unknown",
                processed_files=processed_files,
                metadata=metadata,
                statistics=self.statistics,
                errors=errors,
                processing_time=(datetime.now() - start_time).total_seconds(),
                message=f"Processing failed: {str(e)}",
                output_directory=str(file_output_dir) if file_output_dir else str(self.output_dir),
                preferred_output_root=str(self.preferred_output_base),
                used_output_fallback=self.using_output_fallback
            )
    
    def process_directory(self, directory_path: str, 
                         pattern: str = "*",
                         recursive: bool = True) -> List[ProcessingResult]:
        """
        å¤„ç†ç›®å½•ä¸­çš„æ‰€æœ‰é…ç½®æ–‡ä»¶
        
        Args:
            directory_path: ç›®å½•è·¯å¾„
            pattern: æ–‡ä»¶åŒ¹é…æ¨¡å¼
            recursive: æ˜¯å¦é€’å½’å¤„ç†å­ç›®å½•
            
        Returns:
            å¤„ç†ç»“æœåˆ—è¡¨
        """
        directory_path = Path(directory_path)
        if not directory_path.exists():
            raise ValueError(f"ç›®å½•ä¸å­˜åœ¨: {directory_path}")
        
        # æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„æ–‡ä»¶
        if recursive:
            files = list(directory_path.rglob(pattern))
        else:
            files = list(directory_path.glob(pattern))
        
        logger.info(f"æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶å¾…å¤„ç†")
        
        results = []
        for file_path in tqdm(files, desc="å¤„ç†æ–‡ä»¶"):
            if file_path.is_file():
                result = self.process_file(str(file_path))
                results.append(result)
        
        # ç”Ÿæˆæ€»ä½“æŠ¥å‘Š
        self._generate_summary_report(results)
        
        return results
    
    def _mirror_output_if_needed(self, source_dir: Path, processed_files: List[str]) -> Dict[str, Any]:
        """åœ¨ä¸´æ—¶ç›®å½•å†™å…¥æ—¶å°è¯•åŒæ­¥åˆ°é¡¹ç›® output ç›®å½•"""
        if not self.using_output_fallback or not source_dir:
            return {'mirrored': False}
        
        mirror_info = {'mirrored': False}
        try:
            relative_dir = source_dir.relative_to(self.output_base)
        except ValueError:
            relative_dir = source_dir.name
        
        target_dir = self.preferred_output_base / relative_dir
        
        try:
            target_dir.parent.mkdir(parents=True, exist_ok=True)
            if target_dir.exists():
                shutil.rmtree(target_dir)
            shutil.copytree(source_dir, target_dir)
            
            mirrored_files = []
            for file_path in processed_files:
                try:
                    rel = Path(file_path).relative_to(self.output_base)
                except ValueError:
                    continue
                mirrored_files.append(str(self.preferred_output_base / rel))
            
            mirror_info.update({
                'mirrored': True,
                'mirror_dir': str(target_dir),
                'mirror_files': mirrored_files
            })
        except Exception as exc:
            mirror_info['error'] = str(exc)
        
        return mirror_info
    
    def _unified_to_text(self, unified: Dict) -> str:
        """å°†ç»Ÿä¸€æ ¼å¼è½¬å›æ–‡æœ¬"""
        def dict_to_text(d, indent=0):
            lines = []
            indent_str = "  " * indent
            
            if isinstance(d, dict):
                for key, value in d.items():
                    if isinstance(value, dict):
                        lines.append(f"{indent_str}[{key}]")
                        lines.append(dict_to_text(value, indent + 1))
                    elif isinstance(value, list):
                        lines.append(f"{indent_str}{key} = {','.join(map(str, value))}")
                    else:
                        lines.append(f"{indent_str}{key} = {value}")
            else:
                lines.append(f"{indent_str}{d}")
            
            return '\n'.join(lines)
        
        return dict_to_text(unified.get('config', {}))
    
    def _generate_report(self, **kwargs) -> Dict:
        """ç”Ÿæˆå¤„ç†æŠ¥å‘Š"""
        return {
            'timestamp': datetime.now().isoformat(),
            'version': '1.0.0',
            **kwargs
        }
    
    def _generate_summary_report(self, results: List[ProcessingResult]):
        """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
        summary = {
            'total_files': len(results),
            'successful': sum(1 for r in results if r.success),
            'failed': sum(1 for r in results if not r.success),
            'total_processing_time': sum(r.processing_time for r in results),
            'total_size_mb': self.statistics['total_size_mb'],
            'total_chunks': self.statistics['chunks_created'],
            'files': []
        }
        
        for result in results:
            summary['files'].append({
                'file': result.file_path,
                'success': result.success,
                'format': result.original_format,
                'processing_time': result.processing_time,
                'errors': result.errors
            })
        
        summary_file = self.output_dir / "processing_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜: {summary_file}")




if __name__ == "__main__":
    # ä½¿ç”¨ç¤ºä¾‹
    preprocessor = ConfigPreProcessor("config.yaml")
    
    # å¤„ç†å•ä¸ªæ–‡ä»¶
    result = preprocessor.process_file(
        "sample_config.txt",
        desensitize=True,
        convert_format=True,
        chunk=True,
        extract_metadata=True
    )
    
    if result.success:
        print("âœ… é¢„å¤„ç†æˆåŠŸï¼")
        print(f"å¤„ç†æ—¶é—´: {result.processing_time:.2f} ç§’")
        print(f"è¾“å‡ºæ–‡ä»¶: {len(result.processed_files)} ä¸ª")
        print("\nç”Ÿæˆçš„æ–‡ä»¶:")
        for file in result.processed_files:
            print(f"  - {file}")
    else:
        print("âŒ é¢„å¤„ç†å¤±è´¥ï¼")
        print(f"é”™è¯¯: {result.errors}")
